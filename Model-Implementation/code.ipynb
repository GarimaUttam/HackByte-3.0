{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fce358d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f8ecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator():\n",
    "    def __init__(self, batch_size, target_size=(256, 256)):\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def generate_data(self, subset, directory):\n",
    "\n",
    "        print(\"Loading\", subset, \"...\")\n",
    "\n",
    "        shuffle = False\n",
    "        if subset == 'train' or subset == 'validation':\n",
    "            shuffle = True\n",
    "\n",
    "        dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            directory,\n",
    "            labels='inferred',\n",
    "            label_mode='categorical',\n",
    "            class_names=['0', '1'],\n",
    "            color_mode='rgb',\n",
    "            batch_size=self.batch_size,\n",
    "            image_size=self.target_size,\n",
    "            shuffle=shuffle,\n",
    "            seed=42,\n",
    "        )\n",
    "\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b13be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation_layer(dataset):\n",
    "    data_augmentation = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.RandomFlip('horizontal'),\n",
    "            tf.keras.layers.RandomContrast(0.2),\n",
    "            tf.keras.layers.RandomBrightness(0.2),\n",
    "        ]\n",
    "    )\n",
    "    dataset = dataset.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddcd0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "target_size = (256, 256)\n",
    "data_gen = DataGenerator(batch_size, target_size)\n",
    "\n",
    "train_set = data_gen.generate_data('train', '/kaggle/input/real-vs-ai-generated-faces-dataset/dataset/dataset/train')\n",
    "val_set = data_gen.generate_data('validation', '/kaggle/input/real-vs-ai-generated-faces-dataset/dataset/dataset/validate')\n",
    "test_set = data_gen.generate_data('test', '/kaggle/input/real-vs-ai-generated-faces-dataset/dataset/dataset/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10fcc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([])\n",
    "for x, y in test_set:\n",
    "    labels = np.concatenate([labels, np.argmax(y.numpy(), axis=-1)])\n",
    "\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f7933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = augmentation_layer(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b640f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout\n",
    "from tensorflow.keras.applications import MobileNetV3Large, EfficientNetB2\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "class CustomModels:\n",
    "    \"\"\"Custom Model Class\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=1000, input_shape=(256, 256, 3), name='resnet'):\n",
    "        self.num_classes = num_classes\n",
    "        self.input_shape = input_shape\n",
    "        self.name = name\n",
    "        self.model = None\n",
    "\n",
    "    def build(self):\n",
    "        if \"mobilenet\" in self.name:\n",
    "            base_model = MobileNetV3Large(include_top=False, weights='imagenet')\n",
    "\n",
    "        elif \"efficientnet\" in self.name:\n",
    "            base_model = EfficientNetB2(include_top=False, weights='imagenet')\n",
    "\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        inputs = Input(shape=self.input_shape)\n",
    "        x = base_model(inputs, training=False)\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Dense(units=2048, activation='relu')(x)\n",
    "        outputs = Dense(units=self.num_classes, activation='softmax')(x)\n",
    "        model = Model(inputs=inputs, outputs=outputs, name=self.name)\n",
    "\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def visualize(self):\n",
    "        plot_model(self.model, show_shapes=True, to_file=self.name + '.png')\n",
    "        image = Image(self.name + '.png')\n",
    "        display(image)\n",
    "\n",
    "    def summary(self):\n",
    "        print(self.model.summary(show_trainable=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4173691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, model, loss, optimizer=Adam(learning_rate=0.001), metrics=['accuracy']\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "        self.history = None\n",
    "        self.epochs = 0\n",
    "        self.fine_tune_epochs = 0\n",
    "\n",
    "    def compile(self):\n",
    "        \"\"\"\n",
    "        Method to compile model with optimizer, loss and metrics\n",
    "        \"\"\"\n",
    "        self.model.compile(\n",
    "            optimizer=self.optimizer, loss=self.loss, metrics=self.metrics\n",
    "        )\n",
    "\n",
    "    def fit(self, train_set, val_set, batch_size, epochs, callbacks=[]):\n",
    "        \"\"\"\n",
    "        Method to train model with dataset and return history\n",
    "        \"\"\"\n",
    "        train_history = self.model.fit(\n",
    "            train_set,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=val_set,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "        # history\n",
    "        self.history = train_history.history\n",
    "        self.epochs = len(train_history.history['val_loss'])\n",
    "\n",
    "    def get_model_info(self):\n",
    "        \"\"\"Method to print all informations of model\"\"\"\n",
    "        print(\"- Model name: \", self.model.name)\n",
    "        print(\"- Number of layers: \", len(self.model.layers))\n",
    "        print(\"- Base model name: \", self.model.layers[1].name)\n",
    "        print(\"- Number of layers of base model: \", len(self.model.layers[1].layers))\n",
    "        print(\"- Number of parameters: \", self.model.count_params())\n",
    "\n",
    "    def fine_tune(\n",
    "        self,\n",
    "        train_set,\n",
    "        val_set,\n",
    "        batch_size,\n",
    "        epochs,\n",
    "        new_optimizer,\n",
    "        callbacks=[],\n",
    "        unfreeze_position=0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Method to fine tune model with dataset and return history\n",
    "        \"\"\"\n",
    "        # unfreeze layers\n",
    "        for layer in self.model.layers[1].layers[unfreeze_position:]:\n",
    "            if not isinstance(layer, layers.BatchNormalization):\n",
    "                layer.trainable = True\n",
    "\n",
    "        # recompile model\n",
    "        self.model.compile(\n",
    "            optimizer=new_optimizer, loss=self.loss, metrics=self.metrics\n",
    "        )\n",
    "\n",
    "        print(\"------------\")\n",
    "        print(\n",
    "            f\"Unfreezed from position {unfreeze_position} to {len(self.model.layers[1].layers) - 1}\"\n",
    "        )\n",
    "        print(self.model.summary(show_trainable=True))\n",
    "\n",
    "        output_file = \"/kaggle/working/finetune_layer_info.txt\"\n",
    "        with open(output_file, \"w\") as f:\n",
    "            for i, layer in enumerate(self.model.layers[1].layers):\n",
    "                f.write(f\"{i} {layer.name} {layer.trainable}\\n\")\n",
    "        print(\"------------\")\n",
    "        # print total trainable param of basemodel\n",
    "        total_trainable_param = 0\n",
    "        for layer in self.model.layers[1].layers:\n",
    "            if layer.trainable:\n",
    "                total_trainable_param += layer.count_params()\n",
    "        print(f\"Total trainable param of base model: {total_trainable_param}\")\n",
    "\n",
    "        # fine tune model\n",
    "        fine_tune_history = self.model.fit(\n",
    "            train_set,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=val_set,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "\n",
    "        # add fine tune history to train history\n",
    "        self.history['accuracy'] += fine_tune_history.history['accuracy']\n",
    "        self.history['loss'] += fine_tune_history.history['loss']\n",
    "        self.history['val_accuracy'] += fine_tune_history.history['val_accuracy']\n",
    "        self.history['val_loss'] += fine_tune_history.history['val_loss']\n",
    "\n",
    "        epochs = len(fine_tune_history.history['val_loss'])\n",
    "        self.epochs += epochs\n",
    "        self.fine_tune_epochs += epochs\n",
    "\n",
    "    def plot_history(self):\n",
    "        \"\"\"\n",
    "        Method to plot model training history\n",
    "        \"\"\"\n",
    "\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(16, 8), sharex=True)\n",
    "\n",
    "        axes[0].plot(self.history['accuracy'])\n",
    "        axes[0].plot(self.history['val_accuracy'])\n",
    "        # draw line at fine tune position\n",
    "        if self.fine_tune_epochs > 0:\n",
    "            axes[0].axvline(\n",
    "                x=self.epochs - self.fine_tune_epochs, color='red', linestyle='--'\n",
    "            )\n",
    "\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "\n",
    "        axes[1].plot(self.history['loss'])\n",
    "        axes[1].plot(self.history['val_loss'])\n",
    "\n",
    "        # draw line at fine tune position\n",
    "        if self.fine_tune_epochs > 0:\n",
    "            axes[1].axvline(\n",
    "                x=self.epochs - self.fine_tune_epochs, color='red', linestyle='--'\n",
    "            )\n",
    "\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Loss')\n",
    "\n",
    "        fig.suptitle('Model performance during training')\n",
    "        axes[0].legend(('Train accuracy', 'Validation accuracy'))\n",
    "        axes[1].legend(('Train loss', 'Validation loss'))\n",
    "\n",
    "    def save_model(self, path='/kaggle/working/new_saved_model'):\n",
    "        \"\"\"\n",
    "        Method to save model after training\n",
    "        \"\"\"\n",
    "        save_path = os.path.join(path, f'{self.model.name}.h5')\n",
    "        self.model.save(save_path)\n",
    "        return save_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003008ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "\n",
    "    def __init__(self, model, test_set, y_true):\n",
    "        self.model = model\n",
    "\n",
    "        self.test_loss, self.test_acc = self.model.evaluate(\n",
    "            test_set, verbose=1)\n",
    "\n",
    "        y_pred = self.model.predict(test_set)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        self.cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        self.report = classification_report(y_true, y_pred)\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Method to evaluate model and return loss and accuracy\n",
    "        \"\"\"\n",
    "        return self.test_loss, self.test_acc\n",
    "\n",
    "    def confusion_matrix(self):\n",
    "        \"\"\"\n",
    "        Method to plot confusion matrix\n",
    "        \"\"\"\n",
    "        sns.heatmap(self.cm, annot=True, fmt=\"d\")\n",
    "        plt.show()\n",
    "\n",
    "    def classification_report(self):\n",
    "        \"\"\"\n",
    "        Method to print classification report on test set\n",
    "        \"\"\"\n",
    "        print(self.report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab8ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model = CustomModels(\n",
    "    num_classes=2, input_shape=(256, 256, 3), name='mobilenet_new_50_12'\n",
    ")\n",
    "custom_model.build()\n",
    "custom_model.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d2c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9982ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters for model trainer\n",
    "import math\n",
    "import os\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 256\n",
    "\n",
    "metrics = ['accuracy']\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "initial_learning_rate = 0.01\n",
    "optimizer = tf.keras.optimizers.Adam(initial_learning_rate)\n",
    "\n",
    "def lr_step_decay(epoch, lr):\n",
    "    drop_rate = 0.5\n",
    "    epochs_drop = 5.0\n",
    "    return initial_learning_rate * math.pow(drop_rate, math.floor(epoch / epochs_drop))\n",
    "\n",
    "checkpoint_filepath = os.path.join(\n",
    "    '/kaggle/working/checkpoints',\n",
    "    custom_model.model.name,\n",
    "    \"training\",\n",
    "    \"{epoch:03d}-{val_loss:.4f}.keras\",\n",
    ")\n",
    "\n",
    "os.makedirs(checkpoint_filepath, exist_ok=True)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', mode='min', verbose=1, patience=10\n",
    ")\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(lr_step_decay, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eca517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train model\n",
    "trainer = Trainer(custom_model.model, loss, optimizer, metrics)\n",
    "\n",
    "# Compile model\n",
    "trainer.compile()\n",
    "\n",
    "# Train model\n",
    "trainer.fit(train_set=train_set, val_set=val_set, batch_size=batch_size, epochs=epochs, callbacks=[lr_schedule, early_stopping, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d38778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f68297",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = Evaluator(trainer.model, test_set, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022b4eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d612fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.classification_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58600935",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.get_model_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bb3163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters for finetuning\n",
    "import os\n",
    "import math\n",
    "\n",
    "epochs = 12\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "metrics = ['accuracy']\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "initial_learning_rate = 0.001\n",
    "new_optimizer = tf.keras.optimizers.Adam(initial_learning_rate)\n",
    "\n",
    "def lr_step_decay_2(epoch, lr):\n",
    "    drop_rate = 0.5\n",
    "    epochs_drop = 3.0\n",
    "    return initial_learning_rate * math.pow(drop_rate, math.floor(epoch / epochs_drop))\n",
    "\n",
    "checkpoint_filepath = os.path.join(\n",
    "    '/kaggle/working/checkpoints',\n",
    "    custom_model.model.name,\n",
    "    \"finetuning\",\n",
    "    \"{epoch:03d}-{val_loss:.4f}.keras\",\n",
    "\n",
    ")\n",
    "\n",
    "os.makedirs(checkpoint_filepath, exist_ok=True)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', mode='min', verbose=1, patience=6\n",
    ")\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(lr_step_decay_2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f52c272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune model\n",
    "trainer.fine_tune(train_set=train_set, val_set=val_set, batch_size=batch_size, epochs=epochs, new_optimizer= new_optimizer,\n",
    "                  callbacks=[lr_schedule, early_stopping, checkpoint], unfreeze_position=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd16a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36043d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = Evaluator(trainer.model, test_set, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d377f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3a84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.classification_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30f408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save(\"/kaggle/working/new_saved_model/model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9172539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "def preprocess_image(image_path, target_size=(256, 256)):\n",
    "    img = load_img(image_path, target_size=target_size)\n",
    "    x = img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    return x\n",
    "\n",
    "\n",
    "def predict_image(image_data, model_path):\n",
    "    model = load_model(model_path, compile=False)\n",
    "    prediction = model.predict(image_data)\n",
    "    print(prediction)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "\n",
    "    if predicted_class == 0:\n",
    "        predicted_label = \"real\"\n",
    "    else:\n",
    "        predicted_label = \"fake\"\n",
    "\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0631ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download a fake image to test\n",
    "from IPython.display import Image\n",
    "import requests\n",
    "\n",
    "URL = \"https://cbsaustin.com/resources/media/8b2d4079-8b82-465b-9514-8da9af8d5b49-full1x1_AP23326229180956.jpg\"\n",
    "response = requests.get(URL)\n",
    "\n",
    "image_name = 'real.png'\n",
    "\n",
    "with open(image_name, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "image = Image(image_name)\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b55bcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess image\n",
    "image_name = 'real.png'\n",
    "image = preprocess_image(image_name)\n",
    "model_path = '/kaggle/working/new_saved_model/model.keras'\n",
    "predicted_class = predict_image(image, model_path)\n",
    "print(\"Predicted class:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0196a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download a fake image to test\n",
    "from IPython.display import Image\n",
    "import requests\n",
    "URL = \"https://thispersondoesnotexist.com/\"\n",
    "response = requests.get(URL)\n",
    "\n",
    "image_name = 'fake1.png'\n",
    "\n",
    "with open(image_name, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "image = Image(image_name)\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bad9c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess image\n",
    "image_name = 'fake1.png'\n",
    "image = preprocess_image(image_name)\n",
    "model_path = '/kaggle/working/new_saved_model/model.keras'\n",
    "predicted_class = predict_image(image, model_path)\n",
    "print(\"Predicted class:\", predicted_class)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
